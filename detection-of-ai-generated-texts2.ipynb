{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport json\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:13.844582Z","iopub.execute_input":"2023-04-29T08:14:13.845169Z","iopub.status.idle":"2023-04-29T08:14:19.129392Z","shell.execute_reply.started":"2023-04-29T08:14:13.845097Z","shell.execute_reply":"2023-04-29T08:14:19.128026Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-openai-detector\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-openai-detector\",from_flax=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:19.132141Z","iopub.execute_input":"2023-04-29T08:14:19.133289Z","iopub.status.idle":"2023-04-29T08:14:27.320898Z","shell.execute_reply.started":"2023-04-29T08:14:19.133245Z","shell.execute_reply":"2023-04-29T08:14:27.319884Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/modeling_flax_pytorch_utils.py:384: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_numpy.cpp:199.)\n  pt_model_dict[flax_key] = torch.from_numpy(flax_tensor)\nAll Flax model weights were used when initializing RobertaForSequenceClassification.\n\nSome weights of RobertaForSequenceClassification were not initialized from the Flax model and are newly initialized: ['roberta.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"data = []\n\nwith open(\"/kaggle/input/detection-of-ai-generated-texts/train.jsonl\", \"r\") as file:\n    for line in file:\n        record = json.loads(line)\n        data.append({\"text\": record[\"abstract\"], \"label\": 0})\n        data.append({\"text\": record[\"summary\"], \"label\": 0})\n        data.append({\"text\": record[\"generated\"], \"label\": 1})","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:27.322675Z","iopub.execute_input":"2023-04-29T08:14:27.323153Z","iopub.status.idle":"2023-04-29T08:14:27.363466Z","shell.execute_reply.started":"2023-04-29T08:14:27.323101Z","shell.execute_reply":"2023-04-29T08:14:27.362237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data):\n    tokenized_data = tokenizer(data[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    tokenized_data[\"input_ids\"] = tokenized_data[\"input_ids\"].squeeze(0)\n    tokenized_data[\"attention_mask\"] = tokenized_data[\"attention_mask\"].squeeze(0)\n    return tokenized_data\n\npreprocessed_dataset = [preprocess_data(item) for item in data]","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:27.368502Z","iopub.execute_input":"2023-04-29T08:14:27.368833Z","iopub.status.idle":"2023-04-29T08:14:30.616100Z","shell.execute_reply.started":"2023-04-29T08:14:27.368799Z","shell.execute_reply":"2023-04-29T08:14:30.614889Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:30.618076Z","iopub.execute_input":"2023-04-29T08:14:30.618554Z","iopub.status.idle":"2023-04-29T08:14:30.626862Z","shell.execute_reply.started":"2023-04-29T08:14:30.618505Z","shell.execute_reply":"2023-04-29T08:14:30.625500Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"labels = [item[\"label\"] for item in data]\ndataset = CustomDataset(preprocessed_dataset, labels)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:30.629419Z","iopub.execute_input":"2023-04-29T08:14:30.629873Z","iopub.status.idle":"2023-04-29T08:14:30.643510Z","shell.execute_reply.started":"2023-04-29T08:14:30.629825Z","shell.execute_reply":"2023-04-29T08:14:30.641789Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:30.645974Z","iopub.execute_input":"2023-04-29T08:14:30.646548Z","iopub.status.idle":"2023-04-29T08:14:30.655687Z","shell.execute_reply.started":"2023-04-29T08:14:30.646500Z","shell.execute_reply":"2023-04-29T08:14:30.654460Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Setting Training","metadata":{}},{"cell_type":"code","source":"# Define the accuracy function\ndef compute_accuracy(p):\n    preds = p.predictions.argmax(-1)\n    return {\"accuracy\": accuracy_score(p.label_ids, preds)}","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:30.657476Z","iopub.execute_input":"2023-04-29T08:14:30.658400Z","iopub.status.idle":"2023-04-29T08:14:30.665728Z","shell.execute_reply.started":"2023-04-29T08:14:30.658354Z","shell.execute_reply":"2023-04-29T08:14:30.664346Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    evaluation_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_accuracy,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:30.667361Z","iopub.execute_input":"2023-04-29T08:14:30.668507Z","iopub.status.idle":"2023-04-29T08:14:33.710949Z","shell.execute_reply.started":"2023-04-29T08:14:30.668461Z","shell.execute_reply":"2023-04-29T08:14:33.709714Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:14:33.715874Z","iopub.execute_input":"2023-04-29T08:14:33.716407Z","iopub.status.idle":"2023-04-29T09:26:23.458836Z","shell.execute_reply.started":"2023-04-29T08:14:33.716353Z","shell.execute_reply":"2023-04-29T09:26:23.457184Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myongsuk\u001b[0m (\u001b[33mmaok\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230429_081435-69ylukv1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/maok/huggingface/runs/69ylukv1' target=\"_blank\">clean-frog-7</a></strong> to <a href='https://wandb.ai/maok/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/maok/huggingface' target=\"_blank\">https://wandb.ai/maok/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/maok/huggingface/runs/69ylukv1' target=\"_blank\">https://wandb.ai/maok/huggingface/runs/69ylukv1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2501' max='3965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2501/3965 1:11:02 < 41:37, 0.59 it/s, Epoch 3.15/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.166100</td>\n      <td>0.569916</td>\n      <td>0.678014</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.309700</td>\n      <td>0.222822</td>\n      <td>0.934752</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.225700</td>\n      <td>0.193091</td>\n      <td>0.926241</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:445] . PytorchStreamWriter failed writing file data/166: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_11225/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1638\u001b[0m         )\n\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1979\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1980\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:325] . unexpected pos 1032617344 vs 1032617232"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:325] . unexpected pos 1032617344 vs 1032617232","output_type":"error"}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.460837Z","iopub.status.idle":"2023-04-29T09:26:23.461686Z","shell.execute_reply.started":"2023-04-29T09:26:23.461403Z","shell.execute_reply":"2023-04-29T09:26:23.461438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Sunmission","metadata":{}},{"cell_type":"code","source":"test_data = []\n\nwith open(\"/kaggle/input/detection-of-ai-generated-texts/test.jsonl\", \"r\") as file:\n    for line in file:\n        record = json.loads(line)\n        test_data.append({\"id\":record[\"id\"],\n                     \"abstract\": record[\"abstract\"], \n                     \"summary1\": record[\"summary1\"],\n                     \"summary2\": record[\"summary2\"]})","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.463150Z","iopub.status.idle":"2023-04-29T09:26:23.463983Z","shell.execute_reply.started":"2023-04-29T09:26:23.463672Z","shell.execute_reply":"2023-04-29T09:26:23.463701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test_data(data):\n    return tokenizer(data[\"abstract\"], data[\"summary1\"], return_tensors=\"pt\"), tokenizer(data[\"abstract\"], data[\"summary2\"], return_tensors=\"pt\")\n\npreprocessed_test_data = [preprocess_test_data(item) for item in test_data]","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.465633Z","iopub.status.idle":"2023-04-29T09:26:23.466491Z","shell.execute_reply.started":"2023-04-29T09:26:23.466201Z","shell.execute_reply":"2023-04-29T09:26:23.466232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_summary_class(model, input_data1, input_data2):\n    device = model.device\n    \n    # Move input tensors to the same device as the model\n    input_data1 = {key: tensor.to(device) for key, tensor in input_data1.items()}\n    input_data2 = {key: tensor.to(device) for key, tensor in input_data2.items()}\n    \n    with torch.no_grad():\n        outputs1 = model(**input_data1)\n        outputs2 = model(**input_data2)\n    class1 = torch.argmax(outputs1.logits, dim=-1).item()\n    class2 = torch.argmax(outputs2.logits, dim=-1).item()\n    \n    return 0 if class1 < class2 else 1\n\npredictions = [{\"id\": item[\"id\"], \"class\": predict_summary_class(model, input_data1, input_data2)} for item, (input_data1, input_data2) in zip(test_data, preprocessed_test_data)]","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.468106Z","iopub.status.idle":"2023-04-29T09:26:23.470340Z","shell.execute_reply.started":"2023-04-29T09:26:23.470016Z","shell.execute_reply":"2023-04-29T09:26:23.470049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.472424Z","iopub.status.idle":"2023-04-29T09:26:23.473272Z","shell.execute_reply.started":"2023-04-29T09:26:23.472939Z","shell.execute_reply":"2023-04-29T09:26:23.472982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame({'id':df_predictions[\"id\"],\n                      'answer':df_predictions[\"class\"]})\n\nsubmit.to_csv('submission03-rob-lg.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:26:23.474878Z","iopub.status.idle":"2023-04-29T09:26:23.475810Z","shell.execute_reply.started":"2023-04-29T09:26:23.475545Z","shell.execute_reply":"2023-04-29T09:26:23.475574Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 329, in _process\n    self._sm.send(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 343, in send\n    send_handler(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 363, in send_request\n    logger.debug(f\"send_request: {request_type}\")\nMessage: 'send_request: stop_status'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/filesync/dir_watcher.py\", line 295, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/wandb/run-20230429_081435-69ylukv1/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 280, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/handler.py\", line 136, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/handler.py\", line 144, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\nException in thread OutRawRd-stderr:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1183, in _output_raw_reader_thread\n    self._output_raw_flush(stream)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1198, in _output_raw_flush\n    self._output_raw_file.write(data.encode(\"utf-8\"))\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/filesystem.py\", line 76, in write\n    super().write(b\"\\n\".join(ret) + b\"\\n\")\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/filesystem.py\", line 43, in write\n    self.f.flush()\nOSError: [Errno 28] No space left on device\n\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/filesync/dir_watcher.py\", line 295, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/wandb/run-20230429_081435-69ylukv1/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 332, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1504, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 283, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/handler.py\", line 842, in finish\n    logger.info(\"shutting down handler\")\nMessage: 'shutting down handler'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/service/streams.py\", line 48, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 175, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread SenderThread:'\nArguments: ()\nThread SenderThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 332, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1507, in finish\n    self._output_raw_finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1151, in _output_raw_finish\n    self._output_raw_flush(stream)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/sender.py\", line 1198, in _output_raw_flush\n    self._output_raw_file.write(data.encode(\"utf-8\"))\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/filesystem.py\", line 76, in write\n    super().write(b\"\\n\".join(ret) + b\"\\n\")\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/filesystem.py\", line 43, in write\n    self.f.flush()\nOSError: [Errno 28] No space left on device\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1029, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1009, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.7/threading.py\", line 890, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/filesync/dir_watcher.py\", line 295, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/wandb/run-20230429_081435-69ylukv1/files/output.log'\nArguments: ()\nwandb: ERROR Internal wandb error: file data was not synced\nException in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 281, in check_stop_status\n    process=_process_stop_status,\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 214, in _loop_check_status\n    local_handle = request()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 787, in deliver_stop_status\n    return self._deliver_stop_status(status)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 585, in _deliver_stop_status\n    return self._deliver_record(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n    handle = mailbox._deliver_record(record, interface=self)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n    interface._publish(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n\n","output_type":"stream"}]}]}